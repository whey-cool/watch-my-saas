# Watch My SaaS

## Project Overview

An open-source development intelligence API for vibe coders. You plug it into your GitHub repo via webhook, and it tells you what's actually happening with your AI-assisted development — what percentage is AI-generated, whether quality is trending up or down, and it auto-generates a public "building in public" timeline so you never have to write a changelog again.

The audience builds fast with AI tools (Cursor, Claude Code, Copilot). They want signal, not ceremony.

### The Core: Heuristic Pattern Detection

Metrics and dashboards are table stakes. The real product is **heuristic-based pattern detection from development logs**. After ingesting your commit history, PR patterns, tool transitions, quality signals, and velocity data, Watch My SaaS detects actionable patterns and surfaces them as recommendations. v1 is pure heuristics (no LLM dependency). v2 adds optional BYOK LLM narration.

### Named Patterns

| Pattern | Signal |
|---------|--------|
| **Sprint-Drift Cycle** | AI% high → churn spike → cleanup commits → repeat |
| **Ghost Churn** | Code generated by AI, committed, deleted within days |
| **AI Handoff Cliff** | AI-generated code exceeds developer's review capacity |
| **Tool Transition Spike** | Velocity/churn changes when switching AI tools |
| **Test Coverage Drift** | AI ratio up, test file ratio down |
| **Changelog Silence** | Commits happen but no user-visible changes ship |
| **Workflow Breakthrough** | AI% step function (>15% sustained 2+ weeks) |

### Competitive Positioning

Watch My SaaS is the GPS, not the warning label. GitClear says "AI code is getting worse." We say "here's how to build better with AI." Every competitor targets engineering managers at 20-500+ dev companies. We target the individual builder. We are the first open-source developer intelligence tool focused on AI-augmented workflows. Full analysis: `docs/competitive-landscape-analysis.md`.

### Origin

Developed within the HerdMate ecosystem (`whey-cool/herdmate`). HerdMate provides battle-tested patterns (Hono API, Zod validation, Prisma ORM, RFC 7807 error responses, cursor pagination) and the first dogfood dataset. The founder builds Watch My SaaS to guide her own HerdMate development — HerdMate is the first and most important user. HerdMate itself is never exposed — Watch My SaaS is standalone open-source.

## Decisions Already Made

- **Repo:** `whey-cool/watch-my-saas` (public, clean OSS history)
- **Hosting:** Open-source core, self-hostable via Docker Compose. Full product works self-hosted.
- **Integration:** GitHub webhook + API key
- **Stack:** Hono (API), Zod (validation), Prisma + PostgreSQL (data), Vite + React (dashboard SPA), vanilla TS (embed widget)
- **Auth:** Simple API key (Bearer token). No OAuth, no Clerk.
- **Tooling:** Claude Code as primary dev agent, Copilot Coding Agent for background tasks. `.claude/` config optimized in Session 2 for this stack (Hono/Prisma/vitest/RFC 7807).
- **Recommendation Engine (OQ-1):** Heuristics-only v1. Optional BYOK LLM narration in v2.
- **Deployment (OQ-2):** Docker Compose canonical, Render Blueprint for onboarding. Stateful server, no serverless.
- **AI Tooling (OQ-3):** Claude Code for primary development. Copilot Coding Agent for background tasks via GitHub issues.
- **Report Windowing (OQ-5):** Weekly digest (suppressed if <5 commits) + sprint retrospectives on velocity drop + event-driven public timeline + immediate alerts for revert spikes/coverage drops.
- **Dashboard:** Incremental Vite + React SPA built alongside backend features each session. Feature-flagged (`WATCHMYSAAS_FEATURE_DASHBOARD`). Served from same container (Hono serves static files). Skeleton in Session 4, recommendation views in Session 5, history/timeline in Session 6, public polish in Session 7.

## What NOT to Build

These serve a different buyer (enterprise engineering managers) and dilute focus:

- DORA metrics dashboards
- Developer comparison/ranking
- Jira integration (GitHub Issues if needed)
- Sprint velocity tracking
- SSO/SAML
- Static code analysis (integrate with SonarQube, don't rebuild)
- Developer satisfaction surveys
- 200+ integrations (one done well beats breadth)

## Session History

### Session 1: Git Archaeology Pipeline
Built the three-stage archaeology pipeline (fetch → analyze → wiki) that extracts the development narrative from 1223 commits across 7 whey-cool repos. 5 analyzers (tool transitions, velocity phases, quality evolution, structural growth, unified timeline), 35 tests, 5 wiki pages generated with real data. Added author-identity detection bringing AI commit detection from 47% to 64%.

### Session 2: ECC Optimization & Config Cleanup
Audited and aligned all Claude Code configuration to the project stack. Fixed settings hook format. Deleted 18 irrelevant skills and 6 irrelevant commands. Fixed framework mismatches in 3 skills (jest→vitest, Next.js→Hono, Supabase→Prisma). Zero stale references remaining.

### Session 3: Archaeology Debrief & Decision Session
Brainstorm session validating archaeology findings against developer memory. Confirmed sprint-drift cycle as the core pattern of AI-assisted development. Resolved OQ-1 (heuristics-only v1), OQ-2 (Docker Compose + Render), OQ-3 (Claude Code primary + Copilot background), OQ-5 (hybrid weekly + event windowing). 4 brainstorm pages + 4 decision pages + Architecture.md updated.

### Session 3b: Competitive Analysis & PRD Revision
Startup business analysis of Watch My SaaS. Deep competitive landscape with GitClear focus. OSS-as-product-discovery strategy (telemetry, community, feature flags, dogfood loop). Revised PRD session plan: HerdMate-first development, learning infrastructure from day one, adaptive sessions after first users.

### Session 4a: Serena Onboarding & Session Workflow
Onboarded Serena MCP server for semantic code intelligence. Created 6 memory files (project overview, tech stack, commands, structure, conventions, task checklist). Integrated Serena memory freshness checks into `/session-open` (section 1f) and `/session-close` (section 5b). Decided to commit `.serena/memories/` to the repo for cross-session persistence. Updated session commands to maintain both Claude Code auto memory and Serena memory at session boundaries.

## Next Session

### Session 4: Foundation + Learning Infrastructure

Build the API scaffold, webhook pipeline, database schema, AND the learning infrastructure (telemetry, feature flags, community setup). Resolves OQ-4 (testing strategy). After this session, a GitHub webhook stores classified commits and the project is ready to learn from users.

Key deliverables: Hono API + Prisma schema + webhook pipeline + Pulse telemetry (opt-in) + config-driven feature flags + GitHub Discussions + test suite foundation + dashboard skeleton.

Full spec: `wms-prd.md` → Session 4.

## Open Questions

### OQ-4: Testing the thing that tests things [Session 4]

Testing strategy for a code quality analysis tool. Archaeology findings from Sessions 1 and 3 provide ground truth — the recommendation engine should correctly detect patterns that were manually identified and validated.

## Session Roadmap

```
Session 4: Foundation + Learning Infrastructure
Session 5: Recommendations + HerdMate Goes Live
Session 6: Backfill + Full HerdMate History
    [Personal use phase — weeks on HerdMate]
Session 7: Public Surface + Ship to Others
Session 8+: Adaptive (data-driven)
```

HerdMate-first: the tool must be useful on HerdMate before it ships to anyone else. External users refine the product; personal use validates it.

## Commands

### Wiki Commands

- `/wiki-decision` — Record an architectural/technical decision to the wiki
- `/wiki-changelog` — Append a changelog entry for recent work
- `/wiki-brainstorm` — Capture a brainstorm session to the wiki

### Session Management

- `/session-open` — Documentation freshness audit + session setup. Run at session start.
- `/session-close` — Documentation completeness verification + changelog. Run at session end.
- `/standards` — Review project standards, competitive positioning, and cross-session invariants.

### Planned Commands (built in the session that needs them)

- `/webhook-test` — _(Session 4)_ Send test GitHub webhook payloads to local API, verify classification output.
- `/telemetry-check` — _(Session 4)_ Review Pulse telemetry data at session start (invariant 7).
- `/recommend-validate` — _(Session 5)_ Run recommendation engine against archaeology fixtures, compare to ground truth, report detected/missed/false positive rate.
- `/dogfood` — _(Session 6)_ Check recommendation accuracy log for HerdMate. Report true positive / false positive / useful / noisy breakdown (invariant 9).

### Archaeology Commands

- `npm run archaeology:fetch` — Fetch commit data from local repos + GitHub API
- `npm run archaeology:analyze` — Run all analyzers on raw commit data
- `npm run archaeology:wiki` — Generate wiki pages from analysis results
- `npm run archaeology:all` — Run full pipeline (fetch → analyze → wiki)
- `npm test` — Run analyzer tests (vitest)

## Project Structure

```
watch-my-saas/
├── scripts/
│   ├── wiki.sh                        # Wiki operations wrapper
│   └── archaeology/
│       ├── fetch-commits.sh           # Fetches from local git + GitHub API
│       ├── parse-git-log.mjs          # Parses git log → JSON
│       ├── transform-api-commits.mjs  # Normalizes GitHub API → JSON
│       ├── analyze.ts                 # Orchestrator: runs all analyzers
│       ├── generate-wiki-pages.ts     # Orchestrator: analysis → wiki markdown
│       ├── types.ts                   # Shared TypeScript interfaces
│       ├── analyzers/
│       │   ├── tool-transitions.ts    # Co-Author signature timeline
│       │   ├── velocity-phases.ts     # Weekly commit frequency + phases
│       │   ├── quality-evolution.ts   # Test adoption, churn, reverts
│       │   ├── structural-growth.ts   # Directory timeline, size trajectory
│       │   └── unified-timeline.ts    # Merges all signals chronologically
│       ├── wiki-generators/           # One per wiki page
│       └── __tests__/
│           └── analyzers.test.ts      # 35 tests covering all analyzers
├── docs/
│   └── competitive-landscape-analysis.md  # Full competitive analysis
├── data/archaeology/                  # Gitignored — raw + analysis JSON
├── .claude/
│   ├── commands/                      # Slash commands (wiki-*, session-*, standards)
│   └── hooks/                         # PostToolUse hooks (task state sync)
├── .serena/
│   ├── project.yml                    # Serena MCP project config
│   └── memories/                      # Semantic code intelligence memory (committed)
├── wms-prd.md                         # Product Requirements Document
├── CLAUDE.md                          # This file
├── tsconfig.json
├── package.json
└── README.md
```

## Cross-Session Invariants

1. **Wiki is updated every session.** Decisions → `Decisions/`, session end → `Changelog/`.
2. **CLAUDE.md stays current.** Reflects current project state.
3. **Tests before code.** TDD: write test → fail → implement → pass. Coverage ≥ 80%.
4. **No HerdMate leakage.** Patterns ported, never imported. Archaeology data is gitignored.
5. **Archaeology data is gitignored.** Private repo data never enters the public repo.
6. **The project builds its own history.** Wiki records the full development story.
7. **Telemetry is reviewed every session.** Check Pulse data before planning next session.
8. **Community signals are reviewed every session.** GitHub Discussions and issues, especially "Debugging My Workflow."
9. **Dogfood loop is active.** Every recommendation about HerdMate/watch-my-saas evaluated for accuracy.
10. **Competitive positioning is maintained.** GPS, not warning label. Constructive, never judgmental.
11. **Hard enforcement via hooks.** File path whitelist, TDD gate, test-pass check, and coverage audit are hook-enforced, not just documented.
