# Watch My SaaS

## Project Overview

An open-source development intelligence API for vibe coders. You plug it into your GitHub repo via webhook, and it tells you what's actually happening with your AI-assisted development — what percentage is AI-generated, whether quality is trending up or down, and it auto-generates a public "building in public" timeline so you never have to write a changelog again.

The audience builds fast with AI tools (Cursor, Claude Code, Copilot). They want signal, not ceremony.

### The Core: Heuristic Pattern Detection

Metrics and dashboards are table stakes. The real product is **heuristic-based pattern detection from development logs**. After ingesting your commit history, PR patterns, tool transitions, quality signals, and velocity data, Watch My SaaS detects actionable patterns and surfaces them as recommendations. v1 is pure heuristics (no LLM dependency). v2 adds optional BYOK LLM narration.

### Named Patterns

| Pattern | Signal |
|---------|--------|
| **Sprint-Drift Cycle** | AI% high → churn spike → cleanup commits → repeat |
| **Ghost Churn** | Code generated by AI, committed, deleted within days |
| **AI Handoff Cliff** | AI-generated code exceeds developer's review capacity |
| **Tool Transition Spike** | Velocity/churn changes when switching AI tools |
| **Test Coverage Drift** | AI ratio up, test file ratio down |
| **Changelog Silence** | Commits happen but no user-visible changes ship |
| **Workflow Breakthrough** | AI% step function (>15% sustained 2+ weeks) |

### Competitive Positioning

Watch My SaaS is the GPS, not the warning label. GitClear says "AI code is getting worse." We say "here's how to build better with AI." Every competitor targets engineering managers at 20-500+ dev companies. We target the individual builder. We are the first open-source developer intelligence tool focused on AI-augmented workflows. Full analysis: `docs/competitive-landscape-analysis.md`.

### Origin

Developed within the HerdMate ecosystem (`whey-cool/herdmate`). HerdMate provides battle-tested patterns (Hono API, Zod validation, Prisma ORM, RFC 7807 error responses, cursor pagination) and the first dogfood dataset. The founder builds Watch My SaaS to guide her own HerdMate development — HerdMate is the first and most important user. HerdMate itself is never exposed — Watch My SaaS is standalone open-source.

## Decisions Already Made

- **Repo:** `whey-cool/watch-my-saas` (public, clean OSS history)
- **Hosting:** Open-source core, self-hostable via Docker Compose. Full product works self-hosted.
- **Integration:** GitHub webhook + API key
- **Stack:** Hono (API), Zod (validation), Prisma + PostgreSQL (data), Vite + React (dashboard SPA), vanilla TS (embed widget)
- **Auth:** Simple API key (Bearer token). No OAuth, no Clerk.
- **Tooling:** Claude Code as primary dev agent, Copilot Coding Agent for background tasks. `.claude/` config optimized in Session 2 for this stack (Hono/Prisma/vitest/RFC 7807).
- **Recommendation Engine (OQ-1):** Heuristics-only v1. Optional BYOK LLM narration in v2.
- **Deployment (OQ-2):** Docker Compose canonical, Render Blueprint for onboarding. Stateful server, no serverless.
- **AI Tooling (OQ-3):** Claude Code for primary development. Copilot Coding Agent for background tasks via GitHub issues.
- **Report Windowing (OQ-5):** Weekly digest (suppressed if <5 commits) + sprint retrospectives on velocity drop + event-driven public timeline + immediate alerts for revert spikes/coverage drops.
- **Dashboard:** Incremental Vite + React SPA built alongside backend features each session. Feature-flagged (`WATCHMYSAAS_FEATURE_DASHBOARD`). Served from same container (Hono serves static files). Skeleton in Session 4, recommendation views in Session 5, history/timeline in Session 6, public polish in Session 7.

## What NOT to Build

These serve a different buyer (enterprise engineering managers) and dilute focus:

- DORA metrics dashboards
- Developer comparison/ranking
- Jira integration (GitHub Issues if needed)
- Sprint velocity tracking
- SSO/SAML
- Static code analysis (integrate with SonarQube, don't rebuild)
- Developer satisfaction surveys
- 200+ integrations (one done well beats breadth)

## Session History

### Session 1: Git Archaeology Pipeline
Built the three-stage archaeology pipeline (fetch → analyze → wiki) that extracts the development narrative from 1223 commits across 7 whey-cool repos. 5 analyzers (tool transitions, velocity phases, quality evolution, structural growth, unified timeline), 35 tests, 5 wiki pages generated with real data. Added author-identity detection bringing AI commit detection from 47% to 64%.

### Session 2: ECC Optimization & Config Cleanup
Audited and aligned all Claude Code configuration to the project stack. Fixed settings hook format. Deleted 18 irrelevant skills and 6 irrelevant commands. Fixed framework mismatches in 3 skills (jest→vitest, Next.js→Hono, Supabase→Prisma). Zero stale references remaining.

### Session 3: Archaeology Debrief & Decision Session
Brainstorm session validating archaeology findings against developer memory. Confirmed sprint-drift cycle as the core pattern of AI-assisted development. Resolved OQ-1 (heuristics-only v1), OQ-2 (Docker Compose + Render), OQ-3 (Claude Code primary + Copilot background), OQ-5 (hybrid weekly + event windowing). 4 brainstorm pages + 4 decision pages + Architecture.md updated.

### Session 3b: Competitive Analysis & PRD Revision
Startup business analysis of Watch My SaaS. Deep competitive landscape with GitClear focus. OSS-as-product-discovery strategy (telemetry, community, feature flags, dogfood loop). Revised PRD session plan: HerdMate-first development, learning infrastructure from day one, adaptive sessions after first users.

### Session 4a: Serena Onboarding & Session Workflow
Onboarded Serena MCP server for semantic code intelligence. Created 6 memory files (project overview, tech stack, commands, structure, conventions, task checklist). Integrated Serena memory freshness checks into `/session-open` (section 1f) and `/session-close` (section 5b). Decided to commit `.serena/memories/` to the repo for cross-session persistence. Updated session commands to maintain both Claude Code auto memory and Serena memory at session boundaries.

### Session 4b: Foundation + Learning Infrastructure
First code session. Built the full API scaffold, webhook pipeline, dashboard skeleton, and community infrastructure. Hono API with Zod validation, HMAC-verified webhook endpoint, commit classification service (AI tool detection ported from archaeology — validated on 1223 commits), Prisma schema (Project, Commit, Milestone, QualityReport), opt-in telemetry, feature-flagged dashboard (Vite + React + Tailwind), CI workflow, issue templates, CONTRIBUTING.md. 66 tests, TDD throughout. Resolved OQ-4 (testing strategy: unit + integration + fixtures from archaeology ground truth, 80% coverage threshold). Enforcement pyramid: workflow rules + hook infrastructure + invariant 11.

### Session 5: Recommendations Engine
Built the full recommendation engine (heuristics v1) transforming Watch My SaaS from a data collector into a development GPS. 7 pattern detectors (Sprint-Drift, Ghost Churn, AI Handoff Cliff, Tool Transition, Test Coverage Drift, Changelog Silence, Workflow Breakthrough) with pure-function architecture. Metric aggregation pipeline builds 7-day windows from commit streams. Phase detector synthesizes metrics into project phase (Building/Drifting/Stabilizing/Ship-Ready). Engine orchestrator handles staleness, deduplication, and recommendation lifecycle. 6 new API endpoints (project overview, recommendations CRUD, analyze trigger, reports, timeline). Dashboard upgraded with GPS-style progressive disclosure: project overview (L1), recommendations feed (L2), quality reports (L3). `/recommend-validate` command validates all 7 detectors against synthetic ground truth at 100% accuracy. 161 tests, 97.5% statement coverage, TDD throughout. HerdMate webhook setup deferred to session start of Session 6 (requires live GitHub configuration).

## Next Session

### Session 6: Backfill + Full HerdMate History

Connect HerdMate as the first live project. Set up GitHub webhook on `whey-cool/herdmate`, backfill existing commit history, run the recommendation engine against real data, and begin the dogfood loop. Visual timeline view for the dashboard.

Key deliverables: HerdMate webhook live + commit backfill + dogfood loop active + timeline visualization + `/dogfood` command.

## Open Questions

No open questions. OQ-1 through OQ-5 all resolved.

## Session Roadmap

```
Session 4: Foundation + Learning Infrastructure ← COMPLETE
Session 5: Recommendations Engine ← COMPLETE
Session 6: Backfill + Full HerdMate History + HerdMate Goes Live
    [Personal use phase — weeks on HerdMate]
Session 7: Public Surface + Ship to Others
Session 8+: Adaptive (data-driven)
```

HerdMate-first: the tool must be useful on HerdMate before it ships to anyone else. External users refine the product; personal use validates it.

## Commands

### Wiki Commands

- `/wiki-decision` — Record an architectural/technical decision to the wiki
- `/wiki-changelog` — Append a changelog entry for recent work
- `/wiki-brainstorm` — Capture a brainstorm session to the wiki

### Session Management

- `/session-open` — Documentation freshness audit + session setup. Run at session start.
- `/session-close` — Documentation completeness verification + changelog. Run at session end.
- `/standards` — Review project standards, competitive positioning, and cross-session invariants.

### Verification Commands

- `/webhook-test` — Send test GitHub webhook payloads to local API, verify classification output
- `/telemetry-check` — Review Pulse telemetry state and validate heartbeat payload (invariant 7)
- `/recommend-validate` — Run recommendation engine against synthetic ground truth, report detected/missed/false positive rate per pattern.

### Planned Commands (built in the session that needs them)

- `/dogfood` — _(Session 6)_ Check recommendation accuracy log for HerdMate. Report true positive / false positive / useful / noisy breakdown (invariant 9).

### NPM Scripts

- `npm run dev` — Start API server in watch mode
- `npm test` — Run all tests (vitest)
- `npm run test:coverage` — Run tests with coverage report
- `npm run db:migrate` — Run Prisma migrations
- `npm run telemetry -- status|enable|disable` — Manage telemetry
- `npm run archaeology:all` — Run full archaeology pipeline
- `npm run recommend:validate` — Run recommendation validation against ground truth

## Project Structure

```
watch-my-saas/
├── src/
│   ├── index.ts                       # @hono/node-server entry point
│   ├── app.ts                         # Hono app factory (testable)
│   ├── config.ts                      # Zod-validated env + feature flags
│   ├── types.ts                       # ProblemDetails, ClassifiedCommit, Recommendation, MetricWindow types
│   ├── routes/
│   │   ├── health.ts                  # GET /api/health
│   │   ├── webhooks.ts                # POST /api/webhooks/github (HMAC)
│   │   ├── projects.ts               # GET /api/projects, GET /api/projects/:id overview
│   │   ├── recommendations.ts         # GET/PATCH recommendations, POST analyze
│   │   ├── reports.ts                 # GET /api/projects/:id/reports (paginated)
│   │   └── timeline.ts               # GET /api/projects/:id/timeline
│   ├── middleware/
│   │   ├── auth.ts                    # Bearer token API key
│   │   └── error-handler.ts           # RFC 9457 Problem Details
│   ├── services/
│   │   ├── classification.ts          # Author type + category + quality signals
│   │   ├── webhook-processor.ts       # Extract → classify → store pipeline
│   │   ├── telemetry.ts              # Pulse heartbeat generation
│   │   └── recommendations/           # Pattern detection engine
│   │       ├── engine.ts              # Orchestrator: fetch → aggregate → detect → store
│   │       ├── metrics.ts             # MetricWindow aggregation + trend calculation
│   │       ├── phase-detector.ts      # Project phase synthesis (Building/Drifting/Stabilizing/Ship-Ready)
│   │       └── detectors/             # 7 pure-function pattern detectors
│   │           ├── index.ts           # DETECTORS array export
│   │           ├── sprint-drift.ts
│   │           ├── ghost-churn.ts
│   │           ├── ai-handoff-cliff.ts
│   │           ├── tool-transition.ts
│   │           ├── test-drift.ts
│   │           ├── changelog-silence.ts
│   │           └── workflow-breakthrough.ts
│   ├── db/
│   │   └── client.ts                  # Prisma singleton
│   └── __tests__/                     # 161 tests (vitest)
├── dashboard/                         # Vite + React + Tailwind SPA
│   └── src/
│       ├── pages/                     # Health, Projects, Commits, Overview, Recommendations, Reports
│       └── components/                # Layout, AuthorBadge, PhaseBadge, SeverityBadge, MetricCard
├── prisma/
│   └── schema.prisma                  # Project, Commit, Milestone, QualityReport, Recommendation
├── scripts/
│   ├── telemetry.ts                   # CLI: status | enable | disable
│   ├── recommend-validate.ts          # Recommendation engine ground truth validation
│   ├── wiki.sh                        # Wiki operations wrapper
│   └── archaeology/                   # Fetch + analyze + wiki pipeline
├── .github/
│   ├── workflows/ci.yml               # Test + type check on PR
│   ├── ISSUE_TEMPLATE/                # bug, feature, copilot-task
│   └── DISCUSSION_TEMPLATE/           # debugging-my-workflow
├── .claude/
│   ├── commands/                      # wiki-*, session-*, webhook-test, telemetry-check, recommend-validate
│   ├── hooks/                         # PostToolUse (task state sync)
│   └── rules/workflow.md              # Phase gate protocol, swarm rules
├── docker-compose.yml                 # Postgres + app
├── Dockerfile                         # Multi-stage Node 20 build
├── .env.example                       # All vars documented
├── vitest.config.ts                   # 80% coverage thresholds
└── CONTRIBUTING.md                    # Setup, TDD, PR guidelines
```

## Cross-Session Invariants

1. **Wiki is updated every session.** Decisions → `Decisions/`, session end → `Changelog/`.
2. **CLAUDE.md stays current.** Reflects current project state.
3. **Tests before code.** TDD: write test → fail → implement → pass. Coverage ≥ 80%.
4. **No HerdMate leakage.** Patterns ported, never imported. Archaeology data is gitignored.
5. **Archaeology data is gitignored.** Private repo data never enters the public repo.
6. **The project builds its own history.** Wiki records the full development story.
7. **Telemetry is reviewed every session.** Check Pulse data before planning next session.
8. **Community signals are reviewed every session.** GitHub Discussions and issues, especially "Debugging My Workflow."
9. **Dogfood loop is active.** Every recommendation about HerdMate/watch-my-saas evaluated for accuracy.
10. **Competitive positioning is maintained.** GPS, not warning label. Constructive, never judgmental.
11. **Hard enforcement via hooks.** File path whitelist, TDD gate, test-pass check, and coverage audit are hook-enforced, not just documented.
